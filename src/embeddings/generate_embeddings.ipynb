{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1006f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.11.7)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/HP/Desktop/Beyond-Infinity/venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "import torch\n",
    "from utils import preprocess_text \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_files(folder_path: str) -> List[Document]:\n",
    "    all_docs = []\n",
    "    chunk_counter = 1\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if not filename.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            if \"chunks\" not in data:\n",
    "                print(f\" {filename}: Missing 'chunks' key â€” skipped\")\n",
    "                continue\n",
    "\n",
    "            for chunk in data[\"chunks\"]:\n",
    "                content = chunk.get(\"content\", \"\").strip()\n",
    "                metadata = {\n",
    "                    \"file\": filename,\n",
    "                    \"chunk_id\": str(chunk_counter),\n",
    "                    \"document_type\": chunk.get(\"document_type\", \"\"),\n",
    "                    \"section\": chunk.get(\"section\", \"\"),\n",
    "                    \"source_type\": chunk.get(\"source_type\", \"\"),\n",
    "                    \"source_link\": chunk.get(\"source_link\", \"\"),\n",
    "                }\n",
    "\n",
    "                # Flatten subsections, tags, refs\n",
    "                for key in [\"subsection\", \"tag\", \"references\"]:\n",
    "                    value = chunk.get(key, [])\n",
    "                    if isinstance(value, list):\n",
    "                        for i, v in enumerate(value):\n",
    "                            metadata[f\"{key}_{i+1}\"] = str(v)\n",
    "                    else:\n",
    "                        metadata[f\"{key}_1\"] = str(value)\n",
    "\n",
    "                all_docs.append(Document(page_content=content, metadata=metadata))\n",
    "                chunk_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error in {filename}: {e}\")\n",
    "\n",
    "    print(f\" Loaded {len(all_docs)} documents from {folder_path}\")\n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8eb76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone Index Creation (Hybrid)\n",
    "def create_pinecone_hybrid_index(index_name: str, dimension: int, api_key: str,\n",
    "                                  environment: str = \"us-east-1\"):\n",
    "    pc = Pinecone(api_key=api_key)\n",
    "    existing_indexes = [i.name for i in pc.list_indexes()]\n",
    "\n",
    "    if index_name in existing_indexes:\n",
    "        print(f\" Index '{index_name}' already exists.\")\n",
    "        return pc.Index(index_name)\n",
    "\n",
    "    print(f\"Creating Pinecone HYBRID index '{index_name}'...\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dimension,\n",
    "        metric=\"dotproduct\", \n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=environment),\n",
    "    )\n",
    "\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        print(\" Waiting for index to be ready...\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(f\" Hybrid index '{index_name}' created successfully.\")\n",
    "    return pc.Index(index_name)\n",
    "\n",
    "# Batched Hybrid Uploader\n",
    "def upload_hybrid_in_batches(documents: List[Document],\n",
    "                            dense_embeddings,\n",
    "                            bm25_encoder: BM25Encoder,\n",
    "                            index_name: str,\n",
    "                            api_key: str,\n",
    "                            environment: str = \"us-east-1\",\n",
    "                            batch_size: int = 50):\n",
    "    \"\"\"\n",
    "    Upload documents with both dense and sparse vectors to Pinecone.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of LangChain Document objects\n",
    "        dense_embeddings: HuggingFace embeddings model\n",
    "        bm25_encoder: Fitted BM25Encoder for sparse vectors\n",
    "        index_name: Pinecone index name\n",
    "        api_key: Pinecone API key\n",
    "        environment: Pinecone environment\n",
    "        batch_size: Number of documents per batch\n",
    "    \"\"\"\n",
    "    pc = Pinecone(api_key=api_key)\n",
    "    \n",
    "    # Create index\n",
    "    dimension = len(dense_embeddings.embed_query(\"test\"))\n",
    "    index = create_pinecone_hybrid_index(index_name, dimension, api_key, environment)\n",
    "\n",
    "    total = len(documents)\n",
    "    total_batches = (total + batch_size - 1) // batch_size\n",
    "    print(f\" Uploading {total} docs with HYBRID vectors in {total_batches} batches...\")\n",
    "\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        batch_no = i // batch_size + 1\n",
    "        print(f\" Batch {batch_no}/{total_batches} â€” {len(batch)} docs\")\n",
    "\n",
    "        try:\n",
    "            # Prepare vectors for this batch\n",
    "            vectors_to_upsert = []\n",
    "            \n",
    "            for doc in batch:\n",
    "                # Generate dense vector\n",
    "                dense_vector = dense_embeddings.embed_query(doc.page_content)\n",
    "                \n",
    "                # Generate sparse vector\n",
    "                sparse_vector = bm25_encoder.encode_documents([doc.page_content])[0]\n",
    "                \n",
    "                # Prepare vector entry\n",
    "                vector_entry = {\n",
    "                    \"id\": doc.metadata[\"chunk_id\"],\n",
    "                    \"values\": dense_vector,\n",
    "                    \"sparse_values\": sparse_vector,\n",
    "                    \"metadata\": {\n",
    "                        **doc.metadata,\n",
    "                        \"text\": doc.page_content  # Store text for retrieval\n",
    "                    }\n",
    "                }\n",
    "                vectors_to_upsert.append(vector_entry)\n",
    "            \n",
    "            # Upsert to Pinecone\n",
    "            index.upsert(vectors=vectors_to_upsert)\n",
    "            print(f\" Uploaded batch {batch_no} with hybrid vectors\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error uploading batch {batch_no}: {e}\")\n",
    "            print(\" Retrying in 5s...\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "\n",
    "        # Release memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        time.sleep(2)\n",
    "\n",
    "    print(\" All batches with hybrid vectors uploaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c4d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "âœ… Loaded 102 documents from ../../data/processed/\n",
      "Loading dense embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thebi\\Desktop\\DocsGuide\\src\\projenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\thebi\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting BM25 encoder for sparse vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102/102 [00:00<00:00, 596.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 encoder saved to bm25_params.json\n",
      "\n",
      "ðŸš€ Starting hybrid upload...\n",
      "Creating Pinecone HYBRID index 'nepali-docs-hybrid-multilingual'...\n",
      "âœ… Hybrid index 'nepali-docs-hybrid-multilingual' created successfully.\n",
      "ðŸ“¦ Uploading 102 docs with HYBRID vectors in 3 batches...\n",
      "âž¡ï¸ Batch 1/3 â€” 50 docs\n",
      "âœ… Uploaded batch 1 with hybrid vectors\n",
      "âž¡ï¸ Batch 2/3 â€” 50 docs\n",
      "âœ… Uploaded batch 2 with hybrid vectors\n",
      "âž¡ï¸ Batch 3/3 â€” 2 docs\n",
      "âœ… Uploaded batch 3 with hybrid vectors\n",
      "ðŸŽ‰ All batches with hybrid vectors uploaded successfully!\n",
      "ðŸ Pinecone setup complete!\n"
     ]
    }
   ],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
    "INDEX_NAME = \"nepali-docs-hybrid\"\n",
    "DATA_PATH = \"../../data/processed/\"\n",
    "\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"Please set PINECONE_API_KEY environment variable\")\n",
    "\n",
    "print(\"Loading documents...\")\n",
    "docs = load_json_files(DATA_PATH)\n",
    "\n",
    "print(\"Loading dense embedding model...\")\n",
    "dense_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"universalml/Nepali_Embedding_Model\"\n",
    "    # model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "print(\"Fitting BM25 encoder for sparse vectors...\")\n",
    "bm25_encoder = BM25Encoder()\n",
    "corpus = []\n",
    "\n",
    "for doc in docs:\n",
    "    processed = preprocess_text(doc.page_content)\n",
    "    corpus.append(processed)\n",
    "\n",
    "bm25_encoder.fit(corpus)\n",
    "# Save the fitted BM25 encoder for later use\n",
    "bm25_encoder.dump(\"bm25_params.json\")\n",
    "print(\"BM25 encoder saved to bm25_params.json\")\n",
    "\n",
    "print(\"\\n Starting hybrid upload...\")\n",
    "upload_hybrid_in_batches(\n",
    "    documents=docs,\n",
    "    dense_embeddings=dense_embeddings,\n",
    "    bm25_encoder=bm25_encoder,\n",
    "    index_name=INDEX_NAME,\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENVIRONMENT,\n",
    "    batch_size=50\n",
    ")\n",
    "\n",
    "print(\"ðŸ Pinecone setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9220c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thebi\\Desktop\\DocsGuide\\src\\projenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\thebi\\.cache\\huggingface\\hub\\models--sentence-transformers--LaBSE. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 346])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/LaBSE')\n",
    "tokens = tokenizer(\"à¤¨à¥‡à¤ªà¤¾à¤²à¥€ à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤¤à¤¾ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤—à¤°à¥à¤¨à¤•à¤¾ à¤²à¤¾à¤—à¤¿ à¤¨à¤¯à¤¾à¤ à¤ªà¥à¤°à¤•à¥à¤°à¤¿à¤¯à¤¾ à¤…à¤¨à¥à¤¤à¤°à¥à¤—à¤¤ à¤†à¤µà¤¶à¥à¤¯à¤• à¤ªà¤°à¥à¤¨à¥‡ à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤•à¤¾à¤—à¤œà¤¾à¤¤ à¤¤à¤¥à¤¾ à¤¸à¤¿à¤«à¤¾à¤°à¤¿à¤¸à¤¹à¤°à¥‚à¤•à¥‹ à¤¸à¥‚à¤šà¥€ à¤¨à¤¿à¤®à¥à¤¨ à¤…à¤¨à¥à¤¸à¤¾à¤° à¤°à¤¹à¥‡à¤•à¥‹ à¤›à¥¤ à¤¯à¥€ à¤•à¤¾à¤—à¤œà¤¾à¤¤à¤¹à¤°à¥‚ à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤…à¤µà¤¸à¥à¤¥à¤¾à¤¹à¤°à¥‚à¤®à¤¾ à¤…à¤¨à¤¿à¤µà¤¾à¤°à¥à¤¯ à¤¹à¥à¤¨ à¤¸à¤•à¥à¤›à¤¨à¥à¥¤ à¤…à¤¨à¥à¤¸à¥‚à¤šà¥€-à¥§ à¤•à¥‹ à¤¸à¤¿à¤«à¤¾à¤°à¤¿à¤¸ à¤«à¤¾à¤°à¤¾à¤®à¤®à¤¾ à¤µà¤¡à¤¾ à¤…à¤§à¥à¤¯à¤•à¥à¤·à¤•à¥‹ à¤¸à¤¿à¤«à¤¾à¤°à¤¿à¤¸à¥¤ à¤…à¤¨à¥à¤¸à¥‚à¤šà¥€-à¥§ à¤®à¤¾ à¤Ÿà¤¾à¤à¤¸ à¤­à¤ à¤œà¤¸à¥à¤¤à¥ˆ à¥¨ à¤ªà¥à¤°à¤¤à¤¿ à¤«à¥‹à¤Ÿà¥‹à¥¤ à¤µà¤¿à¤µà¤¾à¤¹ à¤¦à¤°à¥à¤¤à¤¾ à¤ªà¥à¤°à¤®à¤¾à¤£ à¤ªà¤¤à¥à¤°à¥¤ à¤ªà¤¤à¤¿, à¤¬à¥à¤µà¤¾, à¤†à¤®à¤¾, à¤¦à¤¾à¤œà¥, à¤­à¤¾à¤ˆ à¤° à¤›à¥‹à¤°à¤¾à¤•à¥‹ à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤¤à¤¾à¥¤ à¤œà¤¨à¥à¤®à¤¦à¤°à¥à¤¤à¤¾ à¤ªà¥à¤°à¤®à¤¾à¤£-à¤ªà¤¤à¥à¤° à¤µà¤¾ à¤œà¤¨à¥à¤® à¤¸à¥à¤¥à¤¾à¤¨ à¤° à¤œà¤¨à¥à¤® à¤®à¤¿à¤¤à¤¿ à¤à¤•à¤¿à¤¨ à¤¹à¥à¤¨à¥‡ à¤—à¤°à¥€ à¤µà¤¡à¤¾ à¤•à¤¾à¤°à¥à¤¯à¤¾à¤²à¤¯à¤¬à¤¾à¤Ÿ à¤ªà¥à¤°à¤®à¤¾à¤£à¤¿à¤¤ à¤—à¤°à¤¿à¤à¤•à¥‹ à¤¸à¤¿à¤«à¤¾à¤°à¤¿à¤¸ à¤ªà¤¤à¥à¤°à¥¤ à¤µà¤¡à¤¾ à¤®à¥à¤šà¥à¤²à¥à¤•à¤¾ à¤¸à¤¿à¤«à¤¾à¤°à¤¿à¤¸ à¤ªà¤¤à¥à¤° (à¤¨à¤¾à¤¤à¤¾ à¤ªà¤°à¥à¤¨à¥‡ à¤¬à¤¾à¤¹à¥‡à¤•à¤•à¤¾ à¤®à¥à¤šà¥à¤²à¥à¤•à¤¾à¤®à¤¾ à¤¬à¤¸à¥à¤¨à¥‡ à¤µà¥à¤¯à¤•à¥à¤¤à¤¿à¤¹à¤°à¥‚à¤•à¥‹ à¤¨à¤¾à¤® à¤–à¥à¤²à¥‡à¤•à¥‹)à¥¤ à¤ªà¥à¤°à¤¹à¤°à¥€ à¤®à¥à¤šà¥à¤²à¥à¤•à¤¾à¥¤ à¤¬à¥à¤µà¤¾, à¤†à¤®à¤¾, à¤ªà¤¤à¤¿ à¤° à¤ªà¤¤à¥à¤¨à¥€à¤•à¥‹ à¤®à¥ƒà¤¤à¥à¤¯à¥ à¤­à¤à¤•à¥‹ à¤¹à¤•à¤®à¤¾ à¤®à¥ƒà¤¤à¥à¤¯à¥ à¤¦à¤°à¥à¤¤à¤¾ à¤ªà¥à¤°à¤®à¤¾à¤£ à¤ªà¤¤à¥à¤°à¥¤ à¤¬à¤¸à¤¾à¤ˆ à¤¸à¤°à¥€ à¤†à¤à¤•à¥‹ à¤­à¤ à¤¸à¤®à¥à¤¬à¤¨à¥à¤§à¤¿à¤¤ à¤µà¤¡à¤¾à¤•à¥‹ à¤¸à¥à¤¥à¤¾à¤¨à¥€à¤¯ à¤ªà¤žà¥à¤œà¤¿à¤•à¤¾à¤§à¤¿à¤•à¤¾à¤°à¥€à¤¬à¤¾à¤Ÿ à¤œà¤¾à¤°à¥€ à¤¬à¤¸à¤¾à¤ˆà¤¸à¤°à¤¾à¤ˆ à¤ªà¥à¤°à¤®à¤¾à¤£à¤ªà¤¤à¥à¤°à¥¤ à¤¸à¤¨à¤¾à¤–à¤¤ à¤—à¤°à¥à¤¨à¥‡ à¤à¤•à¤¾à¤˜à¤°à¤•à¥‹ à¤µà¥à¤¯à¤•à¥à¤¤à¤¿à¤¸à¤à¤—à¤•à¥‹ à¤¨à¤¾à¤¤à¤¾ à¤ªà¥à¤°à¤®à¤¾à¤£à¤¿à¤¤ à¤ªà¤¤à¥à¤°à¥¤ à¤¬à¥à¤µà¤¾/à¤†à¤®à¤¾à¤¬à¤¾à¤Ÿ à¤¸à¤¨à¤¾à¤–à¤¤ (à¤¬à¥à¤µà¤¾/à¤†à¤®à¤¾ à¤¬à¤¾à¤¹à¥‡à¤• à¤…à¤¨à¥à¤¯ à¤à¤•à¤¾à¤˜à¤°à¤•à¤¾ à¤¸à¤¦à¤¸à¥à¤¯à¤¬à¤¾à¤Ÿ à¤¸à¤¨à¤¾à¤–à¤¤ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤¨à¥‡ à¤¹à¤•à¤®à¤¾ à¤¨à¤¾à¤¤à¤¾ à¤ªà¥à¤°à¤®à¤¾à¤£à¤¿à¤¤ à¤ªà¥à¤°à¤®à¤¾à¤£-à¤ªà¤¤à¥à¤°)à¥¤ à¤µà¤¿à¤µà¤¾à¤¹à¤¿à¤¤ à¤®à¤¹à¤¿à¤²à¤¾à¤•à¥‹ à¤¹à¤•à¤®à¤¾ à¤ªà¤¤à¤¿à¤•à¥‹ à¤¸à¤¨à¤¾à¤–à¤¤, à¤ªà¤¤à¤¿ à¤¨à¤­à¤ˆ à¤¸à¤¾à¤¸à¥/à¤¸à¤¸à¥à¤°à¤¾/à¤œà¥‡à¤ à¤¾à¤œà¥/à¤¦à¥‡à¤µà¤°à¤¬à¤¾à¤Ÿ à¤¸à¤¨à¤¾à¤–à¤¤ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤¨à¥‡ à¤¹à¤•à¤®à¤¾ à¤¨à¤¾à¤¤à¤¾ à¤ªà¥à¤°à¤®à¤¾à¤£à¤¿à¤¤ à¤ªà¥à¤°à¤®à¤¾à¤£-à¤ªà¤¤à¥à¤°à¥¤ à¤¨à¤¾. à¤ªà¥à¤°. à¤ª. à¤•à¥‹ à¤¦à¥à¤µà¥ˆ à¤ªà¥à¤°à¤¤à¤¿ à¤° à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤¤à¤¾à¤•à¥‹ à¤«à¥‹à¤Ÿà¥‹à¤•à¤ªà¥€, à¤®à¤¤à¤¦à¤¾à¤¤à¤¾ à¤ªà¤°à¤¿à¤šà¤¯ à¤ªà¤¤à¥à¤°, à¤¸à¤µà¤¾à¤°à¥€ à¤šà¤¾à¤²à¤• à¤…à¤¨à¥à¤®à¤¤à¤¿ à¤ªà¤¤à¥à¤°, à¤œà¤—à¥à¤—à¤¾à¤§à¤¨à¥€ à¤ªà¥à¤°à¤®à¤¾à¤£ à¤ªà¥à¤°à¥à¤œà¤¾ à¤° à¤¸à¤‚à¤˜ à¤¸à¤‚à¤¸à¥à¤¥à¤¾ à¤µà¤¾ à¤¸à¤°à¤•à¤¾à¤°à¥€ à¤¨à¤¿à¤•à¤¾à¤¯à¤•à¥‹ à¤ªà¤°à¤¿à¤šà¤¯ à¤ªà¤¤à¥à¤°à¥¤ à¤¸à¤®à¥à¤¬à¤¨à¥à¤§ à¤µà¤¿à¤šà¥à¤›à¥‡à¤¦ à¤­à¤à¤•à¥‹ à¤…à¤µà¤¸à¥à¤¥à¤¾à¤®à¤¾ à¤¸à¤®à¥à¤¬à¤¨à¥à¤§ à¤µà¤¿à¤šà¥à¤›à¥‡à¤¦ à¤ªà¥à¤°à¤®à¤¾à¤£-à¤ªà¤¤à¥à¤°, à¤…à¤¦à¤¾à¤²à¤¤à¤•à¥‹ à¤«à¥ˆà¤¸à¤²à¤¾à¤•à¥‹ à¤ªà¥à¤°à¤¤à¤¿à¤²à¤¿à¤ªà¤¿ à¤° à¤µà¤¡à¤¾ à¤•à¤¾à¤°à¥à¤¯à¤¾à¤²à¤¯à¤•à¥‹ à¤¸à¤¿à¤«à¤¾à¤°à¤¿à¤¸ à¤ªà¤¤à¥à¤°à¥¤ à¤¨à¤¾à¤®, à¤¥à¤° à¤µà¤¾ à¤…à¤¨à¥à¤¯ à¤µà¤¿à¤µà¤°à¤£ à¤«à¤°à¤• à¤ªà¤°à¥‡à¤•à¥‹ à¤¹à¤•à¤®à¤¾ à¤«à¤°à¤• à¤ªà¤°à¥‡à¤•à¥‹ à¤µà¥à¤¯à¤•à¥à¤¤à¤¿ à¤à¤‰à¤Ÿà¥ˆ à¤­à¤à¤•à¥‹ à¤­à¤¨à¥à¤¨à¥‡ à¤µà¥à¤¯à¤¹à¥‹à¤°à¤¾à¤•à¥‹ à¤µà¤¡à¤¾ à¤•à¤¾à¤°à¥à¤¯à¤¾à¤²à¤¯à¤•à¥‹ à¤¸à¤¿à¤«à¤¾à¤°à¤¿à¤¸ à¤ªà¤¤à¥à¤°à¥¤\", return_tensors='pt', truncation=False)\n",
    "print(tokens['input_ids'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
